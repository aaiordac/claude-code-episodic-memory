#!/usr/bin/env bash
# episodic-archive: Archive and summarize a Claude Code session
set -euo pipefail

BIN_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$BIN_DIR/../lib/db.sh"
source "$BIN_DIR/../lib/extract.sh"
source "$BIN_DIR/../lib/summarize.sh"
[[ -f "$BIN_DIR/../lib/synthesize.sh" ]] && source "$BIN_DIR/../lib/synthesize.sh"

usage() {
    cat <<EOF
Usage: episodic-archive [OPTIONS] [SESSION_PATH]

Archive a Claude Code session JSONL file.

Options:
  --previous          Archive the most recent session for current project dir
  --current           Archive the current session (uses CLAUDE_SESSION_ID + CWD)
  --catch-up          Summarize all sessions with pending/failed/no_summary status
  --all-in DIR        Archive all sessions in a project directory
  --no-summary        Skip Haiku summary generation (metadata only)
  --dry-run           Show what would be archived without doing it
  -h, --help          Show this help

Examples:
  episodic-archive /path/to/session.jsonl
  episodic-archive --previous
  episodic-archive --current --no-summary
  episodic-archive --catch-up
  episodic-archive --catch-up --dry-run
  episodic-archive --all-in ~/.claude/projects/-Users-foo-myproject/
EOF
}

archive_session() {
    local jsonl_path="$1"
    local skip_summary="${2:-false}"
    local dry_run="${3:-false}"
    local project_override="${4:-}"

    local session_file
    session_file=$(basename "$jsonl_path" .jsonl)
    local project_dir
    project_dir=$(basename "$(dirname "$jsonl_path")")

    # Check if already archived
    if episodic_db_is_archived "$session_file"; then
        episodic_log "INFO" "Session $session_file already archived, skipping"
        return 0
    fi

    episodic_log "INFO" "Archiving session: $session_file"

    # Extract metadata
    local metadata
    metadata=$(episodic_extract_metadata "$jsonl_path")
    if [[ -z "$metadata" ]]; then
        episodic_log "ERROR" "Failed to extract metadata from $jsonl_path"
        return 1
    fi

    local session_id first_prompt message_count user_count assistant_count git_branch created_at modified_at
    session_id=$(echo "$metadata" | jq -r '.session_id')
    first_prompt=$(echo "$metadata" | jq -r '.first_prompt')
    message_count=$(echo "$metadata" | jq -r '.message_count')
    user_count=$(echo "$metadata" | jq -r '.user_message_count')
    assistant_count=$(echo "$metadata" | jq -r '.assistant_message_count')
    git_branch=$(echo "$metadata" | jq -r '.git_branch')
    created_at=$(echo "$metadata" | jq -r '.created_at')
    modified_at=$(echo "$metadata" | jq -r '.modified_at')

    # Use filename as session_id if not found in content
    if [[ "$session_id" == "unknown" || "$session_id" == "null" ]]; then
        session_id="$session_file"
    fi

    # Derive project name (prefer explicit override, e.g. from CWD)
    local project project_path
    if [[ -n "$project_override" ]]; then
        project="$project_override"
    else
        project=$(episodic_project_from_path "$project_dir")
    fi
    project_path=$(episodic_project_path_from_dir "$project_dir")

    # Calculate duration
    local duration=0
    if [[ -n "$created_at" && -n "$modified_at" && "$created_at" != "null" && "$modified_at" != "null" ]]; then
        local start_epoch end_epoch
        # Handle both ISO and epoch timestamps
        if [[ "$created_at" =~ ^[0-9]+\.?[0-9]*$ ]]; then
            start_epoch="${created_at%%.*}"
        else
            # Cross-platform ISO date to epoch
            local ts="${created_at%%.*}"
            if date -j -f "%Y-%m-%dT%H:%M:%S" "$ts" +%s &>/dev/null; then
                start_epoch=$(date -j -f "%Y-%m-%dT%H:%M:%S" "$ts" +%s)
            elif date -d "$ts" +%s &>/dev/null; then
                start_epoch=$(date -d "$ts" +%s)
            else
                start_epoch=0
            fi
        fi
        if [[ "$modified_at" =~ ^[0-9]+\.?[0-9]*$ ]]; then
            end_epoch="${modified_at%%.*}"
        else
            local ts="${modified_at%%.*}"
            if date -j -f "%Y-%m-%dT%H:%M:%S" "$ts" +%s &>/dev/null; then
                end_epoch=$(date -j -f "%Y-%m-%dT%H:%M:%S" "$ts" +%s)
            elif date -d "$ts" +%s &>/dev/null; then
                end_epoch=$(date -d "$ts" +%s)
            else
                end_epoch=0
            fi
        fi

        if [[ "$start_epoch" -gt 0 && "$end_epoch" -gt 0 ]]; then
            # Timestamps might be in milliseconds
            if [[ ${#start_epoch} -gt 12 ]]; then
                start_epoch=$((start_epoch / 1000))
                end_epoch=$((end_epoch / 1000))
            fi
            duration=$(( (end_epoch - start_epoch) / 60 ))
            [[ $duration -lt 0 ]] && duration=0
        fi
    fi

    # Convert epoch timestamps to ISO for storage
    if [[ "$created_at" =~ ^[0-9]+\.?[0-9]*$ ]]; then
        local epoch_s="${created_at%%.*}"
        [[ ${#epoch_s} -gt 12 ]] && epoch_s=$((epoch_s / 1000))
        # Cross-platform epoch to ISO
        if date -r "$epoch_s" -u +"%Y-%m-%dT%H:%M:%SZ" &>/dev/null; then
            created_at=$(date -r "$epoch_s" -u +"%Y-%m-%dT%H:%M:%SZ")
        elif date -u -d "@$epoch_s" +"%Y-%m-%dT%H:%M:%SZ" &>/dev/null; then
            created_at=$(date -u -d "@$epoch_s" +"%Y-%m-%dT%H:%M:%SZ")
        fi
    fi
    if [[ "$modified_at" =~ ^[0-9]+\.?[0-9]*$ ]]; then
        local epoch_s="${modified_at%%.*}"
        [[ ${#epoch_s} -gt 12 ]] && epoch_s=$((epoch_s / 1000))
        if date -r "$epoch_s" -u +"%Y-%m-%dT%H:%M:%SZ" &>/dev/null; then
            modified_at=$(date -r "$epoch_s" -u +"%Y-%m-%dT%H:%M:%SZ")
        elif date -u -d "@$epoch_s" +"%Y-%m-%dT%H:%M:%SZ" &>/dev/null; then
            modified_at=$(date -u -d "@$epoch_s" +"%Y-%m-%dT%H:%M:%SZ")
        fi
    fi

    if [[ "$dry_run" == "true" ]]; then
        echo "Would archive: $session_id"
        echo "  Project: $project"
        echo "  Messages: $message_count (user: $user_count, assistant: $assistant_count)"
        echo "  Duration: ${duration}m"
        echo "  First prompt: ${first_prompt:0:80}..."
        return 0
    fi

    # Copy raw JSONL to archive
    local archive_path="$EPISODIC_ARCHIVE_DIR/$project"
    mkdir -p "$archive_path"
    local archive_file="$archive_path/${session_file}.jsonl"
    if [[ ! -f "$archive_file" ]]; then
        cp "$jsonl_path" "$archive_file"
    fi

    # Insert session into database with 'pending' status.
    # The status only advances to 'complete' after a successful summary,
    # so sessions with failed summaries will be retried on next archive run.
    episodic_db_insert_session \
        "$session_id" "$project" "$project_path" "$archive_file" "$jsonl_path" \
        "$first_prompt" "$message_count" "$user_count" "$assistant_count" \
        "$git_branch" "$created_at" "$modified_at" "$duration"

    episodic_db_update_log "$session_id" "pending"

    # Generate summary unless skipped
    if [[ "$skip_summary" != "true" ]]; then
        local transcript
        transcript=$(episodic_extract "$jsonl_path")

        if [[ -n "$transcript" && ${#transcript} -gt 50 ]]; then
            local summary_json
            summary_json=$(episodic_summarize "$transcript")

            if [[ -n "$summary_json" ]]; then
                episodic_db_insert_summary "$session_id" "$summary_json" "$EPISODIC_SUMMARY_MODEL"
                episodic_db_update_log "$session_id" "complete"
                episodic_log "INFO" "Summary generated for $session_id"
            else
                episodic_db_update_log "$session_id" "summary_failed"
                episodic_log "WARN" "Summary generation failed for $session_id"
            fi
        else
            episodic_db_update_log "$session_id" "too_short"
            episodic_log "INFO" "Session $session_id too short for summary"
        fi
    else
        episodic_db_update_log "$session_id" "no_summary"
    fi

    # Trigger auto-synthesis check
    if type episodic_maybe_synthesize &>/dev/null; then
        episodic_maybe_synthesize "$project" || true
    fi

    echo "Archived: $session_id ($project, ${duration}m, $message_count msgs)"
}

# Main
MODE="single"
SKIP_SUMMARY=false
DRY_RUN=false
TARGET=""

while [[ $# -gt 0 ]]; do
    case "$1" in
        --previous)
            MODE="previous"
            shift
            ;;
        --current)
            MODE="current"
            shift
            ;;
        --catch-up)
            MODE="catchup"
            shift
            ;;
        --all-in)
            MODE="all"
            TARGET="$2"
            shift 2
            ;;
        --no-summary)
            SKIP_SUMMARY=true
            shift
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        -h|--help)
            usage
            exit 0
            ;;
        *)
            TARGET="$1"
            shift
            ;;
    esac
done

case "$MODE" in
    previous)
        # Find current project's session directory
        CWD="${CWD:-$(pwd)}"
        PROJECT_DIR_NAME=$(echo "$CWD" | tr '/' '-')
        SESSION_DIR="$EPISODIC_CLAUDE_PROJECTS/$PROJECT_DIR_NAME"

        if [[ ! -d "$SESSION_DIR" ]]; then
            episodic_log "WARN" "No sessions found for project at $SESSION_DIR"
            exit 0
        fi

        # Find the most recently modified session (excluding current)
        CURRENT_SESSION="${CLAUDE_SESSION_ID:-}"
        LATEST=""
        while IFS= read -r f; do
            fname=$(basename "$f" .jsonl)
            if [[ "$fname" != "$CURRENT_SESSION" ]]; then
                LATEST="$f"
                break
            fi
        done < <(ls -t "$SESSION_DIR"/*.jsonl 2>/dev/null)

        if [[ -z "$LATEST" ]]; then
            episodic_log "INFO" "No previous sessions to archive"
            exit 0
        fi

        # Use basename of CWD as project name (reliable, avoids lossy encoding)
        CWD_PROJECT=$(basename "$CWD")
        archive_session "$LATEST" "$SKIP_SUMMARY" "$DRY_RUN" "$CWD_PROJECT"
        ;;

    current)
        # Archive the current session by ID (used by stop hook)
        CWD="${CWD:-$(pwd)}"
        CURRENT_SESSION="${CLAUDE_SESSION_ID:-}"

        if [[ -z "$CURRENT_SESSION" ]]; then
            episodic_log "INFO" "CLAUDE_SESSION_ID not set, skipping --current"
            exit 0
        fi

        PROJECT_DIR_NAME=$(echo "$CWD" | tr '/' '-')
        SESSION_FILE="$EPISODIC_CLAUDE_PROJECTS/$PROJECT_DIR_NAME/${CURRENT_SESSION}.jsonl"

        if [[ ! -f "$SESSION_FILE" ]]; then
            episodic_log "INFO" "Session file not found: $SESSION_FILE"
            exit 0
        fi

        # Detect resumed sessions: if message count grew, reset status so
        # archive_session re-processes it and catch-up re-summarizes it.
        CURRENT_MSG_COUNT=$(jq -s 'length' "$SESSION_FILE" 2>/dev/null || echo 0)
        STORED_MSG_COUNT=$(episodic_db_exec "SELECT COALESCE(message_count, 0) FROM sessions WHERE id='$(episodic_sql_escape "$CURRENT_SESSION")';" 2>/dev/null || echo 0)
        STORED_MSG_COUNT="${STORED_MSG_COUNT:-0}"
        if [[ "$STORED_MSG_COUNT" -gt 0 && "$CURRENT_MSG_COUNT" -gt "$STORED_MSG_COUNT" ]]; then
            episodic_log "INFO" "Session $CURRENT_SESSION resumed: $STORED_MSG_COUNT -> $CURRENT_MSG_COUNT messages, re-archiving"
            # Reset status so episodic_db_is_archived returns false
            episodic_db_update_log "$CURRENT_SESSION" "pending"
        fi

        CWD_PROJECT=$(basename "$CWD")
        archive_session "$SESSION_FILE" "$SKIP_SUMMARY" "$DRY_RUN" "$CWD_PROJECT"
        ;;

    catchup)
        # Cross-project catch-up: summarize sessions with retryable status
        # Queries archive_log for sessions needing summaries (no_summary, summary_failed, pending)
        episodic_db_init "$EPISODIC_DB" >/dev/null 2>&1

        # Find sessions that need summaries
        CATCHUP_ROWS=$(episodic_db_query_json "
            SELECT s.id, s.project, s.source_path, s.archive_path, al.status
            FROM sessions s
            JOIN archive_log al ON al.session_id = s.id
            WHERE al.status IN ('no_summary', 'summary_failed', 'pending')
            ORDER BY s.created_at ASC;
        ")

        CATCHUP_COUNT=$(echo "$CATCHUP_ROWS" | jq 'length' 2>/dev/null || echo 0)

        if [[ "$CATCHUP_COUNT" -eq 0 || "$CATCHUP_ROWS" == "[]" ]]; then
            episodic_log "INFO" "Catch-up: no sessions need summaries"
            exit 0
        fi

        episodic_log "INFO" "Catch-up: $CATCHUP_COUNT sessions need summaries"

        # Track projects that got new summaries for synthesis
        declare -A CATCHUP_PROJECTS

        for i in $(seq 0 $((CATCHUP_COUNT - 1))); do
            ROW=$(echo "$CATCHUP_ROWS" | jq -r ".[$i]")
            SID=$(echo "$ROW" | jq -r '.id')
            SPROJ=$(echo "$ROW" | jq -r '.project')
            SPATH=$(echo "$ROW" | jq -r '.source_path')
            APATH=$(echo "$ROW" | jq -r '.archive_path')
            SSTATUS=$(echo "$ROW" | jq -r '.status')

            if [[ "$DRY_RUN" == "true" ]]; then
                echo "Would summarize: $SID (project=$SPROJ, status=$SSTATUS)"
                continue
            fi

            # Find the transcript file: prefer source_path, fall back to archive_path
            TFILE=""
            if [[ -n "$SPATH" && "$SPATH" != "null" && -f "$SPATH" ]]; then
                TFILE="$SPATH"
            elif [[ -n "$APATH" && "$APATH" != "null" && -f "$APATH" ]]; then
                TFILE="$APATH"
            fi

            if [[ -z "$TFILE" ]]; then
                episodic_log "WARN" "Catch-up: no transcript found for $SID"
                continue
            fi

            # Extract and summarize
            TRANSCRIPT=$(episodic_extract "$TFILE" 2>/dev/null || true)

            if [[ -z "$TRANSCRIPT" || ${#TRANSCRIPT} -le 50 ]]; then
                episodic_db_update_log "$SID" "too_short"
                episodic_log "INFO" "Catch-up: $SID too short for summary"
            else
                SUMMARY_JSON=$(episodic_summarize "$TRANSCRIPT" 2>/dev/null || true)

                if [[ -n "$SUMMARY_JSON" ]]; then
                    episodic_db_insert_summary "$SID" "$SUMMARY_JSON" "$EPISODIC_SUMMARY_MODEL"
                    episodic_db_update_log "$SID" "complete"
                    episodic_log "INFO" "Catch-up: summary generated for $SID"
                    CATCHUP_PROJECTS["$SPROJ"]=1
                    echo "Summarized: $SID ($SPROJ)"
                else
                    episodic_db_update_log "$SID" "summary_failed"
                    episodic_log "WARN" "Catch-up: summary failed for $SID"
                fi
            fi

            # Rate limit between API calls
            sleep 0.5
        done

        # Trigger synthesis once per project that got new summaries
        if [[ "$DRY_RUN" != "true" ]] && type episodic_maybe_synthesize &>/dev/null; then
            for PROJ in "${!CATCHUP_PROJECTS[@]}"; do
                episodic_maybe_synthesize "$PROJ" || true
            done
        fi

        if [[ "$DRY_RUN" == "true" ]]; then
            echo "Catch-up dry run: $CATCHUP_COUNT sessions would be processed"
        else
            SUMMARIZED=${#CATCHUP_PROJECTS[@]}
            episodic_log "INFO" "Catch-up finished: $CATCHUP_COUNT processed, $SUMMARIZED projects got new summaries"
        fi
        ;;

    all)
        if [[ ! -d "$TARGET" ]]; then
            echo "ERROR: Directory not found: $TARGET" >&2
            exit 1
        fi
        count=0
        for f in "$TARGET"/*.jsonl; do
            [[ -f "$f" ]] || continue
            # Skip subagent directories
            [[ "$f" == *"/subagents/"* ]] && continue
            archive_session "$f" "$SKIP_SUMMARY" "$DRY_RUN"
            count=$((count + 1))
        done
        echo "Processed $count sessions"
        ;;

    single)
        if [[ -z "$TARGET" ]]; then
            usage
            exit 1
        fi
        archive_session "$TARGET" "$SKIP_SUMMARY" "$DRY_RUN"
        ;;
esac
